{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3816,"databundleVersionId":32105,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install dagshub mlflow --quiet\n\nimport warnings\nfrom statsmodels.tools.sm_exceptions import ValueWarning\n\nwarnings.filterwarnings(\"ignore\", category=ValueWarning)\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T18:14:32.561575Z","iopub.execute_input":"2025-07-03T18:14:32.561891Z","iopub.status.idle":"2025-07-03T18:14:48.201507Z","shell.execute_reply.started":"2025-07-03T18:14:32.561864Z","shell.execute_reply":"2025-07-03T18:14:48.200471Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m733.8/733.8 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.5/95.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nydata-profiling 4.16.1 requires dacite>=1.8, but you have dacite 1.6.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mDone!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T18:16:31.368251Z","iopub.execute_input":"2025-07-03T18:16:31.368623Z","iopub.status.idle":"2025-07-03T18:16:31.376896Z","shell.execute_reply.started":"2025-07-03T18:16:31.368586Z","shell.execute_reply":"2025-07-03T18:16:31.375978Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\n/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, FunctionTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nimport mlflow\nimport mlflow.sklearn\nimport dagshub\nimport joblib\nimport os\nfrom sklearn.model_selection import GridSearchCV\nfrom tqdm import tqdm\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\ndagshub.init(repo_owner='gnada22', repo_name='ml_final_project', mlflow=True)\n\nmlflow.set_experiment(\"SARIMA_Training\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DateFeatureCreator(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        X[\"week\"] = (\n            X[\"Date\"].dt.to_period(\"W\")\n            .rank(method=\"dense\")\n            .astype(int) - 1\n        )\n        X[\"sin_13\"] = np.sin(2 * np.pi * X[\"week\"] / 13)\n        X[\"cos_13\"] = np.cos(2 * np.pi * X[\"week\"] / 13)\n        X[\"sin_23\"] = np.sin(2 * np.pi * X[\"week\"] / 23)\n        X[\"cos_23\"] = np.cos(2 * np.pi * X[\"week\"] / 23)\n        X = X.drop(columns=[\"Date\"])\n        return X\n\ndate_features = [\"week\", \"sin_13\", \"cos_13\", \"sin_23\", \"cos_23\"]\n\nlag_features = []\n\nclass ColumnDropper(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        return X.drop(columns=self.columns, errors=\"ignore\")\n\nadded_features = date_features + lag_features\n\nclass ColumnTransformerWithNames(ColumnTransformer):\n    def get_feature_names_out(self, input_features=None):\n        return super().get_feature_names_out(input_features)\n\n    def transform(self, X):\n        X_transformed = super().transform(X)\n        cols = self.get_feature_names_out()\n        cols = [c.split(\"__\", 1)[-1] for c in self.get_feature_names_out()]\n        res = pd.DataFrame(X_transformed, columns=cols, index=X.index)\n        return res\n\n    def fit_transform(self, X, y=None):\n        X_transformed = super().fit_transform(X, y)\n        cols = self.get_feature_names_out()\n        cols = [c.split(\"__\", 1)[-1] for c in self.get_feature_names_out()]\n        res = pd.DataFrame(X_transformed, columns=cols, index=X.index)\n        return res\n\nclass MultiIndexKeeper(BaseEstimator, TransformerMixin):\n    def __init__(self, index_cols=[\"Date\", \"Store\", \"Dept\"]):\n        self.index_cols = index_cols\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        X.set_index(self.index_cols, drop=False, inplace=True)\n        return X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T18:16:44.038745Z","iopub.execute_input":"2025-07-03T18:16:44.039037Z","iopub.status.idle":"2025-07-03T18:16:44.051444Z","shell.execute_reply.started":"2025-07-03T18:16:44.039013Z","shell.execute_reply":"2025-07-03T18:16:44.050426Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class SARIMARegressor(BaseEstimator, RegressorMixin):\n    def __init__(self, order=(1, 0, 0), seasonal_order=(1, 0, 0, 52), store_level='Store', dept_level='Dept'):\n        self.order = order\n        self.seasonal_order = seasonal_order\n        self.store_level = store_level\n        self.dept_level = dept_level\n\n    def fit(self, X, y):\n        if not isinstance(X.index, pd.MultiIndex):\n            raise ValueError(\"X must have a MultiIndex\")\n\n        self.models_ = {}\n        self.avgs_ = {}\n\n        df = X.copy()\n        df[\"target\"] = y.values\n\n        grouped = df.groupby(level=[self.store_level, self.dept_level])\n\n        for (store, dept), group_df in grouped:\n            # print(\"store, dept: \", store, dept)\n            if dept == 1:\n                print(\"store: \", store)\n            group_df = group_df.sort_index(level=\"Date\")\n\n            dates = pd.to_datetime(group_df.index.get_level_values(\"Date\"))\n            dates = dates.sort_values()\n            dates = pd.DatetimeIndex(dates)\n            \n            ts = group_df[\"target\"].copy()\n            exog = group_df.drop(columns=[\"target\"])\n\n            ts.index = dates\n            exog.index = dates\n\n            if ts is None or len(ts) < (self.order[0] + self.order[2] + 1) or ts.isna().all():\n                print(f\"Skipping Store {store}, Dept {dept} due to insufficient data\")\n                self.skip_(ts, store, dept)\n                continue\n\n            if ts is None or len(ts) < 3:\n                self.skip_(ts, store, dept)\n                continue\n            \n            ts = ts.dropna()\n            \n            # Skip if constant values (no variance)\n            if ts.nunique() == 1:\n                print(f\"Skipping ({store}, {dept}) - constant series\")\n                self.skip_(ts, store, dept)\n                continue\n            \n            # check standard deviation\n            if ts.std() < 1e-6:\n                print(f\"Skipping ({store}, {dept}) - nearly constant series\")\n                self.skip_(ts, store, dept)\n                continue\n            try:\n                model = SARIMAX(\n                    endog=ts,\n                    order=self.order,\n                    seasonal_order=self.seasonal_order,\n                    enforce_stationarity=False,\n                    enforce_invertibility=False\n                ).fit(disp=False)\n                self.models_[(store, dept)] = model\n            except (ValueError, np.linalg.LinAlgError) as e:\n                # Skip problematic groups\n                print(f\"Skipping ({store}, {dept}) due to error: {e}\")\n                self.skip_(ts, store, dept)\n                continue\n\n        return self\n\n    def skip_(self, ts, store, dept):\n        if ts is None or len(ts) == 0:\n            self.avgs_[(store, dept)] = 0.0\n        else:\n            self.avgs_[(store, dept)] = ts.mean()\n\n    def predict(self, X):\n        if not isinstance(X.index, pd.MultiIndex):\n            raise ValueError(\"X must have a MultiIndex\")\n    \n        preds = pd.Series(index=X.index, dtype=float)\n    \n        # Group X by store-dept pair (based on index levels)\n        grouped = X.groupby(level=[self.store_level, self.dept_level])\n    \n        for (store, dept), group_df in grouped:\n            if dept == 1:\n                print(\"store: \", store)\n            model = self.models_.get((store, dept))\n            if model is None:\n                preds.loc[group_df.index] = self.avgs_.get((store, dept), 0)\n                continue\n\n            group_df = group_df.sort_index(level=\"Date\")\n\n            dates = pd.to_datetime(group_df.index.get_level_values(\"Date\"))\n            dates = dates.sort_values()\n            dates = pd.DatetimeIndex(dates)\n\n            # exog = group_df.copy()\n            # exog.index = dates\n    \n            # Forecast N steps = number of rows in this group\n            forecast = model.forecast(steps=len(group_df))\n            preds.loc[group_df.index] = forecast.to_numpy()\n    \n        return preds.to_numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T18:17:34.697609Z","iopub.execute_input":"2025-07-03T18:17:34.697904Z","iopub.status.idle":"2025-07-03T18:17:34.713548Z","shell.execute_reply.started":"2025-07-03T18:17:34.697881Z","shell.execute_reply":"2025-07-03T18:17:34.712654Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Load and merge data\ndf_train = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\", parse_dates=[\"Date\"])\ndf_features = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\", parse_dates=[\"Date\"])\ndf_stores = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\")\n\ndf = df_train.merge(df_features, on=[\"Store\", \"Date\", \"IsHoliday\"], how=\"left\")\ndf = df.merge(df_stores, on=\"Store\", how=\"left\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T18:18:53.740954Z","iopub.execute_input":"2025-07-03T18:18:53.741355Z","iopub.status.idle":"2025-07-03T18:18:54.181924Z","shell.execute_reply.started":"2025-07-03T18:18:53.741319Z","shell.execute_reply":"2025-07-03T18:18:54.181081Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"categorical = [\"Store\", \"Dept\", \"Type\", \"IsHoliday\"]\nnumerical = [\"Temperature\", \"Fuel_Price\", \"CPI\", \"Unemployment\",\n             \"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\"]\nengineered = added_features\n\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\nnumerical_transformer = SimpleImputer(strategy=\"mean\")\n\npreprocessor = ColumnTransformerWithNames(\n    transformers=[\n        (\"num\", numerical_transformer, numerical + engineered),\n        (\"cat\", categorical_transformer, categorical),\n    ],\n    sparse_threshold=0.0\n)\n\npipeline = Pipeline([\n    (\"index_keeper\", MultiIndexKeeper()),\n    (\"date_features\", DateFeatureCreator()),\n    (\"preprocessor\", preprocessor),\n    (\"column_dropper\", ColumnDropper(columns=lag_features)),\n    (\"sarima\", SARIMARegressor(order=(5,1,0)))\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T18:19:03.445871Z","iopub.execute_input":"2025-07-03T18:19:03.446173Z","iopub.status.idle":"2025-07-03T18:19:03.452438Z","shell.execute_reply.started":"2025-07-03T18:19:03.446151Z","shell.execute_reply":"2025-07-03T18:19:03.451475Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"with mlflow.start_run(run_name=\"Feature_Engineering\"):\n    mlflow.log_param(\"features_added\", added_features)\n\ny = df[\"Weekly_Sales\"]\nX = df.drop(columns=[\"Weekly_Sales\"])\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T18:19:06.570848Z","iopub.execute_input":"2025-07-03T18:19:06.571152Z","iopub.status.idle":"2025-07-03T18:19:07.736834Z","shell.execute_reply.started":"2025-07-03T18:19:06.571126Z","shell.execute_reply":"2025-07-03T18:19:07.735848Z"}},"outputs":[{"name":"stdout","text":"ðŸƒ View run Feature_Engineering at: https://dagshub.com/gnada22/ml_final_project.mlflow/#/experiments/4/runs/b759fd74d5b7438d99138c20efcc469f\nðŸ§ª View experiment at: https://dagshub.com/gnada22/ml_final_project.mlflow/#/experiments/4\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# print(X_train.columns)\n\nwith mlflow.start_run(run_name=\"Training\"):\n    # grid_search.fit(X_train, y_train)\n    # best_model = grid_search.best_estimator_\n    # preds = best_model.predict(X_val)\n\n    # mlflow.log_params(grid_search.best_params_)\n\n    pipeline.fit(X_train, y_train)\n    preds = pipeline.predict(X_val)\n    best_model = pipeline\n\n    mae = mean_absolute_error(y_val, preds)\n    weights = X_val[\"IsHoliday\"].apply(lambda x: 5 if x else 1)\n    wmae = (weights * np.abs(y_val - preds)).sum() / weights.sum()\n\n    mlflow.log_metric(\"MAE\", mae)\n    mlflow.log_metric(\"WMAE\", wmae)\n\n    model_path = \"model.pkl\"\n    joblib.dump(best_model, model_path)\n    mlflow.log_artifact(model_path)\n    \n    # mlflow.sklearn.log_model(best_model, artifact_path=\"model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T18:19:11.025765Z","iopub.execute_input":"2025-07-03T18:19:11.026057Z","iopub.status.idle":"2025-07-03T18:27:23.430913Z","shell.execute_reply.started":"2025-07-03T18:19:11.026035Z","shell.execute_reply":"2025-07-03T18:27:23.429725Z"}},"outputs":[{"name":"stdout","text":"store:  1\nSkipping Store 1, Dept 77 due to insufficient data\nSkipping Store 1, Dept 78 due to insufficient data\nstore:  2\nSkipping Store 2, Dept 39 due to insufficient data\nSkipping Store 2, Dept 77 due to insufficient data\nstore:  3\nSkipping Store 3, Dept 77 due to insufficient data\nSkipping Store 3, Dept 78 due to insufficient data\nstore:  4\nstore:  5\nSkipping Store 5, Dept 51 due to insufficient data\nSkipping Store 5, Dept 77 due to insufficient data\nSkipping Store 5, Dept 78 due to insufficient data\nSkipping Store 5, Dept 80 due to insufficient data\nstore:  6\nðŸƒ View run Training at: https://dagshub.com/gnada22/ml_final_project.mlflow/#/experiments/4/runs/5a931476e49347919922fdbb9726ffc9\nðŸ§ª View experiment at: https://dagshub.com/gnada22/ml_final_project.mlflow/#/experiments/4\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3969307456.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# mlflow.log_params(grid_search.best_params_)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/2685131382.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0menforce_stationarity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                     \u001b[0menforce_invertibility\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 ).fit(disp=False)\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdept\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinAlgError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/statespace/mlemodel.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, start_params, transformed, includes_fixed, cov_type, cov_kwds, method, maxiter, full_output, disp, callback, return_params, optim_score, optim_complex_step, optim_hessian, flags, low_memory, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m             res = func(mlefit.params, transformed=False, includes_fixed=False,\n\u001b[0m\u001b[1;32m    730\u001b[0m                        cov_type=cov_type, cov_kwds=cov_kwds)\n\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/statespace/mlemodel.py\u001b[0m in \u001b[0;36msmooth\u001b[0;34m(self, params, transformed, includes_fixed, complex_step, cov_type, cov_kwds, return_ssm, results_class, results_wrapper_class, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# Get the state space output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomplex_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcomplex_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0;31m# Wrap in a results object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/statespace/kalman_smoother.py\u001b[0m in \u001b[0;36msmooth\u001b[0;34m(self, smoother_output, smooth_method, results, run_filter, prefix, complex_step, update_representation, update_filter, update_smoother, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msmoother_output\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0msmoother_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmoother_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0msmoother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_smooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmoother_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;31m# Update the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/statsmodels/tsa/statespace/kalman_smoother.py\u001b[0m in \u001b[0;36m_smooth\u001b[0;34m(self, smoother_output, smooth_method, prefix, complex_step, results, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;31m# Run the smoother\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0msmoother\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msmoother\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":12}]}