{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3816,"databundleVersionId":32105,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install dagshub mlflow --quiet\n\nimport warnings\nfrom statsmodels.tools.sm_exceptions import ValueWarning\n\nwarnings.filterwarnings(\"ignore\", category=ValueWarning)\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T13:20:47.991817Z","iopub.execute_input":"2025-07-04T13:20:47.992093Z","iopub.status.idle":"2025-07-04T13:21:04.117067Z","shell.execute_reply.started":"2025-07-04T13:20:47.992066Z","shell.execute_reply":"2025-07-04T13:21:04.116057Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m733.8/733.8 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m95.5/95.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nydata-profiling 4.16.1 requires dacite>=1.8, but you have dacite 1.6.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mDone!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T13:21:54.147202Z","iopub.execute_input":"2025-07-04T13:21:54.147594Z","iopub.status.idle":"2025-07-04T13:21:54.154904Z","shell.execute_reply.started":"2025-07-04T13:21:54.147526Z","shell.execute_reply":"2025-07-04T13:21:54.153861Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\n/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, FunctionTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nimport mlflow\nimport mlflow.sklearn\nimport dagshub\nimport joblib\nimport os\nfrom sklearn.model_selection import GridSearchCV\nfrom tqdm import tqdm\nfrom statsmodels.tsa.arima.model import ARIMA\n\ndagshub.init(repo_owner='gnada22', repo_name='ml_final_project', mlflow=True)\n\nmlflow.set_experiment(\"ARIMA_Training\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T13:21:55.940197Z","iopub.execute_input":"2025-07-04T13:21:55.940574Z","iopub.status.idle":"2025-07-04T13:22:04.090817Z","shell.execute_reply.started":"2025-07-04T13:21:55.940547Z","shell.execute_reply":"2025-07-04T13:22:04.089674Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                                       \u001b[1m‚ùó‚ùó‚ùó AUTHORIZATION REQUIRED ‚ùó‚ùó‚ùó\u001b[0m                                        \n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">‚ùó‚ùó‚ùó AUTHORIZATION REQUIRED ‚ùó‚ùó‚ùó</span>                                        \n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n\nOpen the following link in your browser to authorize the client:\nhttps://dagshub.com/login/oauth/authorize?state=be7ede0e-de7f-4c41-870d-d25989b50c33&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=7148caa834521caeccf3c5631c9ddc2bbbe39f9b5eacb49a201303c9006f59b3\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Accessing as gnada22\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as gnada22\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Initialized MLflow to track repo \u001b[32m\"gnada22/ml_final_project\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"gnada22/ml_final_project\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Repository gnada22/ml_final_project initialized!\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository gnada22/ml_final_project initialized!\n</pre>\n"},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<Experiment: artifact_location='mlflow-artifacts:/dfedb6aa3d5842948e6ecfc54df65b9e', creation_time=1751556649691, experiment_id='3', last_update_time=1751556649691, lifecycle_stage='active', name='ARIMA_Training', tags={}>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"class DateFeatureCreator(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        X[\"week\"] = (\n            X[\"Date\"].dt.to_period(\"W\")\n            .rank(method=\"dense\")\n            .astype(int) - 1\n        )\n        X[\"sin_13\"] = np.sin(2 * np.pi * X[\"week\"] / 13)\n        X[\"cos_13\"] = np.cos(2 * np.pi * X[\"week\"] / 13)\n        X[\"sin_23\"] = np.sin(2 * np.pi * X[\"week\"] / 23)\n        X[\"cos_23\"] = np.cos(2 * np.pi * X[\"week\"] / 23)\n        X = X.drop(columns=[\"Date\"])\n        return X\n\ndate_features = [\"week\", \"sin_13\", \"cos_13\", \"sin_23\", \"cos_23\"]\n\nlag_features = []\n\nclass ColumnDropper(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        return X.drop(columns=self.columns, errors=\"ignore\")\n\nadded_features = date_features + lag_features\n\nclass ColumnTransformerWithNames(ColumnTransformer):\n    def get_feature_names_out(self, input_features=None):\n        return super().get_feature_names_out(input_features)\n\n    def transform(self, X):\n        X_transformed = super().transform(X)\n        cols = self.get_feature_names_out()\n        cols = [c.split(\"__\", 1)[-1] for c in self.get_feature_names_out()]\n        res = pd.DataFrame(X_transformed, columns=cols, index=X.index)\n        return res\n\n    def fit_transform(self, X, y=None):\n        X_transformed = super().fit_transform(X, y)\n        cols = self.get_feature_names_out()\n        cols = [c.split(\"__\", 1)[-1] for c in self.get_feature_names_out()]\n        res = pd.DataFrame(X_transformed, columns=cols, index=X.index)\n        return res\n\nclass MultiIndexKeeper(BaseEstimator, TransformerMixin):\n    def __init__(self, index_cols=[\"Date\", \"Store\", \"Dept\"]):\n        self.index_cols = index_cols\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        X.set_index(self.index_cols, drop=False, inplace=True)\n        return X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T13:22:11.162553Z","iopub.execute_input":"2025-07-04T13:22:11.162909Z","iopub.status.idle":"2025-07-04T13:22:11.174950Z","shell.execute_reply.started":"2025-07-04T13:22:11.162884Z","shell.execute_reply":"2025-07-04T13:22:11.173839Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class ARIMARegressor(BaseEstimator, RegressorMixin):\n    def __init__(self, order=(1, 0, 0), store_level='Store', dept_level='Dept'):\n        self.order = order\n        self.store_level = store_level\n        self.dept_level = dept_level\n\n    def fit(self, X, y):\n        if not isinstance(X.index, pd.MultiIndex):\n            raise ValueError(\"X must have a MultiIndex\")\n\n        self.models_ = {}\n        self.avgs_ = {}\n\n        df = X.copy()\n        df[\"target\"] = y.values\n\n        grouped = df.groupby(level=[self.store_level, self.dept_level])\n\n        for (store, dept), group_df in grouped:\n            if dept == 1:\n                print(\"store: \", store)\n    \n            ts = group_df[\"target\"].copy()\n            exog = group_df.drop(columns=[\"target\"])\n        \n            try:\n                model = ARIMA(endog=ts, order=self.order).fit()\n                self.models_[(store, dept)] = model\n            except Exception as e:\n                # Skip problematic groups\n                print(f\"Skipping ({store}, {dept}) due to error: {e}\")\n                self.skip_(ts, store, dept)\n                continue\n\n        return self\n\n    def skip_(self, ts, store, dept):\n        if ts is None or len(ts) == 0:\n            self.avgs_[(store, dept)] = 0.0\n        else:\n            self.avgs_[(store, dept)] = ts.mean()\n\n    def predict(self, X):\n        if not isinstance(X.index, pd.MultiIndex):\n            raise ValueError(\"X must have a MultiIndex\")\n    \n        preds = pd.Series(index=X.index, dtype=float)\n    \n        # Group X by store-dept pair (based on index levels)\n        grouped = X.groupby(level=[self.store_level, self.dept_level])\n    \n        for (store, dept), group_df in grouped:\n            if dept == 1:\n                print(\"store: \", store)\n            model = self.models_.get((store, dept))\n            if model is None:\n                preds.loc[group_df.index] = self.avgs_.get((store, dept), 0)\n                continue\n\n            # exog = group_df.copy()\n    \n            # Forecast N steps = number of rows in this group\n            forecast = model.forecast(steps=len(group_df))\n            preds.loc[group_df.index] = forecast.to_numpy()\n    \n        return preds.to_numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T13:22:12.431193Z","iopub.execute_input":"2025-07-04T13:22:12.431473Z","iopub.status.idle":"2025-07-04T13:22:12.443658Z","shell.execute_reply.started":"2025-07-04T13:22:12.431453Z","shell.execute_reply":"2025-07-04T13:22:12.442797Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"categorical = [\"Store\", \"Dept\", \"Type\", \"IsHoliday\"]\nnumerical = [\"Temperature\", \"Fuel_Price\", \"CPI\", \"Unemployment\",\n             \"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\"]\nengineered = added_features\n\ncategorical_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\nnumerical_transformer = SimpleImputer(strategy=\"mean\")\n\npreprocessor = ColumnTransformerWithNames(\n    transformers=[\n        (\"num\", numerical_transformer, numerical + engineered),\n        (\"cat\", categorical_transformer, categorical),\n    ],\n    sparse_threshold=0.0\n)\n\npipeline = Pipeline([\n    (\"index_keeper\", MultiIndexKeeper()),\n    (\"date_features\", DateFeatureCreator()),\n    (\"preprocessor\", preprocessor),\n    (\"column_dropper\", ColumnDropper(columns=lag_features)),\n    (\"arima\", ARIMARegressor(order=(5,1,0)))\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T13:22:15.319118Z","iopub.execute_input":"2025-07-04T13:22:15.319414Z","iopub.status.idle":"2025-07-04T13:22:15.326047Z","shell.execute_reply.started":"2025-07-04T13:22:15.319390Z","shell.execute_reply":"2025-07-04T13:22:15.325125Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Load and merge data\ndf_train = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\", parse_dates=[\"Date\"])\ndf_features = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\", parse_dates=[\"Date\"])\ndf_stores = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\")\n\ndf = df_train.merge(df_features, on=[\"Store\", \"Date\", \"IsHoliday\"], how=\"left\")\ndf = df.merge(df_stores, on=\"Store\", how=\"left\")\ndf = df.sort_values(by=['Date', 'Store', 'Dept'])\n\nwith mlflow.start_run(run_name=\"Feature_Engineering\"):\n    mlflow.log_param(\"features_added\", added_features)\n\ny = df[\"Weekly_Sales\"]\nX = df.drop(columns=[\"Weekly_Sales\"])\n\nsplit_idx = int(len(X) * 0.8)\nX_train, X_val = X.iloc[:split_idx], X.iloc[split_idx:]\ny_train, y_val = y.iloc[:split_idx], y.iloc[split_idx:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T13:22:17.564164Z","iopub.execute_input":"2025-07-04T13:22:17.564448Z","iopub.status.idle":"2025-07-04T13:22:18.913338Z","shell.execute_reply.started":"2025-07-04T13:22:17.564428Z","shell.execute_reply":"2025-07-04T13:22:18.912346Z"}},"outputs":[{"name":"stdout","text":"üèÉ View run Feature_Engineering at: https://dagshub.com/gnada22/ml_final_project.mlflow/#/experiments/3/runs/de37cc48ff6c4472ba5832a0673edc5a\nüß™ View experiment at: https://dagshub.com/gnada22/ml_final_project.mlflow/#/experiments/3\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"with mlflow.start_run(run_name=\"Training\"):\n    pipeline.fit(X_train, y_train)\n    preds = pipeline.predict(X_val)\n    best_model = pipeline\n\n    mae = mean_absolute_error(y_val, preds)\n    weights = X_val[\"IsHoliday\"].apply(lambda x: 5 if x else 1)\n    wmae = (weights * np.abs(y_val - preds)).sum() / weights.sum()\n\n    mlflow.log_metric(\"MAE\", mae)\n    mlflow.log_metric(\"WMAE\", wmae)\n\n    model_path = \"model.pkl\"\n    joblib.dump(best_model, model_path)\n    mlflow.log_artifact(model_path)\n    \n    # mlflow.sklearn.log_model(best_model, artifact_path=\"model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T13:22:20.723132Z","iopub.execute_input":"2025-07-04T13:22:20.723438Z","iopub.status.idle":"2025-07-04T13:31:36.434113Z","shell.execute_reply.started":"2025-07-04T13:22:20.723413Z","shell.execute_reply":"2025-07-04T13:31:36.432813Z"}},"outputs":[{"name":"stdout","text":"store:  1\nstore:  2\nSkipping (2, 77) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nstore:  3\nSkipping (3, 47) due to error: LU decomposition error.\nSkipping (3, 78) due to error: Schur decomposition solver error.\nstore:  4\nSkipping (4, 39) due to error: Schur decomposition solver error.\nstore:  5\nSkipping (5, 77) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nSkipping (5, 78) due to error: Schur decomposition solver error.\nstore:  6\nSkipping (6, 77) due to error: Schur decomposition solver error.\nstore:  7\nSkipping (7, 78) due to error: Schur decomposition solver error.\nstore:  8\nstore:  9\nSkipping (9, 77) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nSkipping (9, 78) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nSkipping (9, 93) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nstore:  10\nSkipping (10, 77) due to error: Schur decomposition solver error.\nstore:  11\nstore:  12\nstore:  13\nSkipping (13, 43) due to error: Schur decomposition solver error.\nSkipping (13, 77) due to error: Schur decomposition solver error.\nstore:  14\nSkipping (14, 43) due to error: Schur decomposition solver error.\nstore:  15\nSkipping (15, 37) due to error: Schur decomposition solver error.\nSkipping (15, 43) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nSkipping (15, 48) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nSkipping (15, 60) due to error: LU decomposition error.\nstore:  16\nSkipping (16, 77) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nSkipping (16, 78) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nstore:  17\nstore:  18\nSkipping (18, 39) due to error: Schur decomposition solver error.\nSkipping (18, 48) due to error: Schur decomposition solver error.\nSkipping (18, 99) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nstore:  19\nstore:  20\nstore:  21\nSkipping (21, 48) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nSkipping (21, 50) due to error: Schur decomposition solver error.\nSkipping (21, 77) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nSkipping (21, 96) due to error: Schur decomposition solver error.\nstore:  22\nSkipping (22, 99) due to error: Schur decomposition solver error.\nstore:  23\nstore:  24\nstore:  25\nSkipping (25, 48) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nSkipping (25, 77) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nstore:  26\nSkipping (26, 78) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nstore:  27\nSkipping (27, 39) due to error: Schur decomposition solver error.\nstore:  28\nSkipping (28, 43) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nstore:  29\nSkipping (29, 96) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nstore:  30\nSkipping (30, 19) due to error: Schur decomposition solver error.\nSkipping (30, 33) due to error: Schur decomposition solver error.\nstore:  31\nstore:  32\nSkipping (32, 77) due to error: Schur decomposition solver error.\nstore:  33\nSkipping (33, 27) due to error: Schur decomposition solver error.\nSkipping (33, 71) due to error: Schur decomposition solver error.\nstore:  34\nSkipping (34, 77) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nSkipping (34, 78) due to error: Schur decomposition solver error.\nstore:  35\nSkipping (35, 96) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nstore:  36\nSkipping (36, 29) due to error: Schur decomposition solver error.\nSkipping (36, 33) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nSkipping (36, 72) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nSkipping (36, 85) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nstore:  37\nSkipping (37, 71) due to error: Schur decomposition solver error.\nSkipping (37, 99) due to error: Schur decomposition solver error.\nstore:  38\nSkipping (38, 35) due to error: Schur decomposition solver error.\nSkipping (38, 99) due to error: Schur decomposition solver error.\nstore:  39\nSkipping (39, 78) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nstore:  40\nSkipping (40, 47) due to error: LU decomposition error.\nSkipping (40, 78) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nstore:  41\nSkipping (41, 37) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nstore:  42\nSkipping (42, 33) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nSkipping (42, 34) due to error: Schur decomposition solver error.\nSkipping (42, 41) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nstore:  43\nSkipping (43, 24) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nSkipping (43, 55) due to error: Schur decomposition solver error.\nSkipping (43, 71) due to error: Schur decomposition solver error.\nstore:  44\nSkipping (44, 24) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nSkipping (44, 33) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nSkipping (44, 34) due to error: Schur decomposition solver error.\nSkipping (44, 49) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nSkipping (44, 71) due to error: Schur decomposition solver error.\nSkipping (44, 99) due to error: too many indices for array: array is 0-dimensional, but 1 were indexed\nstore:  45\nSkipping (45, 96) due to error: Schur decomposition solver error.\nstore:  1\nstore:  2\nstore:  3\nstore:  4\nstore:  5\nstore:  6\nstore:  7\nstore:  8\nstore:  9\nstore:  10\nstore:  11\nstore:  12\nstore:  13\nstore:  14\nstore:  15\nstore:  16\nstore:  17\nstore:  18\nstore:  19\nstore:  20\nstore:  21\nstore:  22\nstore:  23\nstore:  24\nstore:  25\nstore:  26\nstore:  27\nstore:  28\nstore:  29\nstore:  30\nstore:  31\nstore:  32\nstore:  33\nstore:  34\nstore:  35\nstore:  36\nstore:  37\nstore:  38\nstore:  39\nstore:  40\nstore:  41\nstore:  42\nstore:  43\nstore:  44\nstore:  45\nüèÉ View run Training at: https://dagshub.com/gnada22/ml_final_project.mlflow/#/experiments/3/runs/8e98d08cee9a4a78859be10e6c0f561c\nüß™ View experiment at: https://dagshub.com/gnada22/ml_final_project.mlflow/#/experiments/3\n","output_type":"stream"}],"execution_count":9}]}