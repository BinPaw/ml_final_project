{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3816,"databundleVersionId":32105,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install dagshub mlflow --quiet\n\nimport warnings\nfrom statsmodels.tools.sm_exceptions import ValueWarning\n\nwarnings.filterwarnings(\"ignore\", category=ValueWarning)\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T16:43:37.883204Z","iopub.execute_input":"2025-07-03T16:43:37.883631Z","iopub.status.idle":"2025-07-03T16:43:42.376630Z","shell.execute_reply.started":"2025-07-03T16:43:37.883603Z","shell.execute_reply":"2025-07-03T16:43:42.375329Z"}},"outputs":[{"name":"stdout","text":"Done!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-03T16:38:41.823385Z","iopub.execute_input":"2025-07-03T16:38:41.823801Z","iopub.status.idle":"2025-07-03T16:38:41.832125Z","shell.execute_reply.started":"2025-07-03T16:38:41.823764Z","shell.execute_reply":"2025-07-03T16:38:41.830911Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\n/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, FunctionTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nimport mlflow\nimport mlflow.sklearn\nimport dagshub\nimport joblib\nimport os\nfrom sklearn.model_selection import GridSearchCV\nfrom tqdm import tqdm\nfrom statsmodels.tsa.arima.model import ARIMA\n\ndagshub.init(repo_owner='gnada22', repo_name='ml_final_project', mlflow=True)\n\nmlflow.set_experiment(\"ARIMA_Training\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T16:38:43.528160Z","iopub.execute_input":"2025-07-03T16:38:43.528504Z","iopub.status.idle":"2025-07-03T16:38:53.543684Z","shell.execute_reply.started":"2025-07-03T16:38:43.528478Z","shell.execute_reply":"2025-07-03T16:38:53.542885Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                                       \u001b[1m❗❗❗ AUTHORIZATION REQUIRED ❗❗❗\u001b[0m                                        \n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">❗❗❗ AUTHORIZATION REQUIRED ❗❗❗</span>                                        \n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n\nOpen the following link in your browser to authorize the client:\nhttps://dagshub.com/login/oauth/authorize?state=6ab6d896-b410-42d4-bdd3-57ad22f68204&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=1f567a1bb9af35747cadde30a5640726afcdc6e4449eba0576236899a02f5ccb\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Accessing as gnada22\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as gnada22\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Initialized MLflow to track repo \u001b[32m\"gnada22/ml_final_project\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"gnada22/ml_final_project\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Repository gnada22/ml_final_project initialized!\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository gnada22/ml_final_project initialized!\n</pre>\n"},"metadata":{}},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"<Experiment: artifact_location='mlflow-artifacts:/dfedb6aa3d5842948e6ecfc54df65b9e', creation_time=1751556649691, experiment_id='3', last_update_time=1751556649691, lifecycle_stage='active', name='ARIMA_Training', tags={}>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"class DateFeatureCreator(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        X[\"week\"] = (\n            X[\"Date\"].dt.to_period(\"W\")\n            .rank(method=\"dense\")\n            .astype(int) - 1\n        )\n        X[\"sin_13\"] = np.sin(2 * np.pi * X[\"week\"] / 13)\n        X[\"cos_13\"] = np.cos(2 * np.pi * X[\"week\"] / 13)\n        X[\"sin_23\"] = np.sin(2 * np.pi * X[\"week\"] / 23)\n        X[\"cos_23\"] = np.cos(2 * np.pi * X[\"week\"] / 23)\n        X = X.drop(columns=[\"Date\"])\n        return X\n\ndate_features = [\"week\", \"sin_13\", \"cos_13\", \"sin_23\", \"cos_23\"]\n\nlag_features = []\n\nclass ColumnDropper(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        return X.drop(columns=self.columns, errors=\"ignore\")\n\nadded_features = date_features + lag_features\n\nclass ColumnTransformerWithNames(ColumnTransformer):\n    def get_feature_names_out(self, input_features=None):\n        return super().get_feature_names_out(input_features)\n\n    def transform(self, X):\n        X_transformed = super().transform(X)\n        cols = self.get_feature_names_out()\n        cols = [c.split(\"__\", 1)[-1] for c in self.get_feature_names_out()]\n        res = pd.DataFrame(X_transformed, columns=cols, index=X.index)\n        return res\n\n    def fit_transform(self, X, y=None):\n        X_transformed = super().fit_transform(X, y)\n        cols = self.get_feature_names_out()\n        cols = [c.split(\"__\", 1)[-1] for c in self.get_feature_names_out()]\n        res = pd.DataFrame(X_transformed, columns=cols, index=X.index)\n        return res\n\nclass MultiIndexKeeper(BaseEstimator, TransformerMixin):\n    def __init__(self, index_cols=[\"Date\", \"Store\", \"Dept\"]):\n        self.index_cols = index_cols\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        X.set_index(self.index_cols, drop=False, inplace=True)\n        return X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T16:38:57.657027Z","iopub.execute_input":"2025-07-03T16:38:57.657396Z","iopub.status.idle":"2025-07-03T16:38:57.671381Z","shell.execute_reply.started":"2025-07-03T16:38:57.657371Z","shell.execute_reply":"2025-07-03T16:38:57.670142Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class ARIMARegressor(BaseEstimator, RegressorMixin):\n    def __init__(self, order=(1, 0, 0), store_level='Store', dept_level='Dept'):\n        self.order = order\n        self.store_level = store_level\n        self.dept_level = dept_level\n\n    def fit(self, X, y):\n        if not isinstance(X.index, pd.MultiIndex):\n            raise ValueError(\"X must have a MultiIndex\")\n\n        self.models_ = {}\n        self.avgs_ = {}\n\n        df = X.copy()\n        df[\"target\"] = y.values\n\n        grouped = df.groupby(level=[self.store_level, self.dept_level])\n\n        for (store, dept), group_df in grouped:\n            # print(\"store, dept: \", store, dept)\n            if dept == 1:\n                print(\"store: \", store)\n            group_df = group_df.sort_index(level=\"Date\")\n\n            dates = pd.to_datetime(group_df.index.get_level_values(\"Date\"))\n            dates = dates.sort_values()\n            dates = pd.DatetimeIndex(dates)\n            \n            ts = group_df[\"target\"].copy()\n            exog = group_df.drop(columns=[\"target\"])\n\n            ts.index = dates\n            exog.index = dates\n\n            if ts is None or len(ts) < (self.order[0] + self.order[2] + 1) or ts.isna().all():\n                print(f\"Skipping Store {store}, Dept {dept} due to insufficient data\")\n                self.skip_(ts, store, dept)\n                continue\n\n            if ts is None or len(ts) < 3:\n                self.skip_(ts, store, dept)\n                continue\n            \n            ts = ts.dropna()\n            \n            # Skip if constant values (no variance)\n            if ts.nunique() == 1:\n                print(f\"Skipping ({store}, {dept}) - constant series\")\n                self.skip_(ts, store, dept)\n                continue\n            \n            # check standard deviation\n            if ts.std() < 1e-6:\n                print(f\"Skipping ({store}, {dept}) - nearly constant series\")\n                self.skip_(ts, store, dept)\n                continue\n            try:\n                model = ARIMA(endog=ts, order=self.order).fit()\n                self.models_[(store, dept)] = model\n            except (ValueError, np.linalg.LinAlgError) as e:\n                # Skip problematic groups\n                print(f\"Skipping ({store}, {dept}) due to error: {e}\")\n                self.skip_(ts, store, dept)\n                continue\n\n        return self\n\n    def skip_(self, ts, store, dept):\n        if ts is None or len(ts) == 0:\n            self.avgs_[(store, dept)] = 0.0\n        else:\n            self.avgs_[(store, dept)] = ts.mean()\n\n    def predict(self, X):\n        if not isinstance(X.index, pd.MultiIndex):\n            raise ValueError(\"X must have a MultiIndex\")\n    \n        preds = pd.Series(index=X.index, dtype=float)\n    \n        # Group X by store-dept pair (based on index levels)\n        grouped = X.groupby(level=[self.store_level, self.dept_level])\n    \n        for (store, dept), group_df in grouped:\n            if dept == 1:\n                print(\"store: \", store)\n            model = self.models_.get((store, dept))\n            if model is None:\n                preds.loc[group_df.index] = self.avgs_.get((store, dept), 0)\n                continue\n\n            group_df = group_df.sort_index(level=\"Date\")\n\n            dates = pd.to_datetime(group_df.index.get_level_values(\"Date\"))\n            dates = dates.sort_values()\n            dates = pd.DatetimeIndex(dates)\n\n            # exog = group_df.copy()\n            # exog.index = dates\n    \n            # Forecast N steps = number of rows in this group\n            forecast = model.forecast(steps=len(group_df))\n            preds.loc[group_df.index] = forecast.to_numpy()\n    \n        return preds.to_numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T17:11:28.968721Z","iopub.execute_input":"2025-07-03T17:11:28.969157Z","iopub.status.idle":"2025-07-03T17:11:28.988273Z","shell.execute_reply.started":"2025-07-03T17:11:28.969130Z","shell.execute_reply":"2025-07-03T17:11:28.987080Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# Load and merge data\ndf_train = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\", parse_dates=[\"Date\"])\ndf_features = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\", parse_dates=[\"Date\"])\ndf_stores = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\")\n\ndf = df_train.merge(df_features, on=[\"Store\", \"Date\", \"IsHoliday\"], how=\"left\")\ndf = df.merge(df_stores, on=\"Store\", how=\"left\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T16:39:03.181895Z","iopub.execute_input":"2025-07-03T16:39:03.182244Z","iopub.status.idle":"2025-07-03T16:39:03.802146Z","shell.execute_reply.started":"2025-07-03T16:39:03.182208Z","shell.execute_reply":"2025-07-03T16:39:03.801144Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"categorical = [\"Store\", \"Dept\", \"Type\", \"IsHoliday\"]\nnumerical = [\"Temperature\", \"Fuel_Price\", \"CPI\", \"Unemployment\",\n             \"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\"]\nengineered = added_features\n\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\nnumerical_transformer = SimpleImputer(strategy=\"mean\")\n\npreprocessor = ColumnTransformerWithNames(\n    transformers=[\n        (\"num\", numerical_transformer, numerical + engineered),\n        (\"cat\", categorical_transformer, categorical),\n    ],\n    sparse_threshold=0.0\n)\n\npipeline = Pipeline([\n    (\"index_keeper\", MultiIndexKeeper()),\n    (\"date_features\", DateFeatureCreator()),\n    (\"preprocessor\", preprocessor),\n    (\"column_dropper\", ColumnDropper(columns=lag_features)),\n    (\"arima\", ARIMARegressor(order=(5,1,0)))\n])\n\nparam_grid = {\n    \"arima__order\": [(1,1,1), (2,1,2), (3,1,0), (5,1,0)],\n}\n\ngrid_search = GridSearchCV(\n    estimator=pipeline,\n    param_grid=param_grid,\n    scoring=\"neg_mean_absolute_error\",\n    cv=3,\n    n_jobs=1,\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T17:11:36.413084Z","iopub.execute_input":"2025-07-03T17:11:36.413434Z","iopub.status.idle":"2025-07-03T17:11:36.421768Z","shell.execute_reply.started":"2025-07-03T17:11:36.413408Z","shell.execute_reply":"2025-07-03T17:11:36.420555Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"with mlflow.start_run(run_name=\"Feature_Engineering\"):\n    mlflow.log_param(\"features_added\", added_features)\n\ny = df[\"Weekly_Sales\"]\nX = df.drop(columns=[\"Weekly_Sales\"])\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T16:39:06.760239Z","iopub.execute_input":"2025-07-03T16:39:06.760577Z","iopub.status.idle":"2025-07-03T16:39:08.329511Z","shell.execute_reply.started":"2025-07-03T16:39:06.760552Z","shell.execute_reply":"2025-07-03T16:39:08.328626Z"}},"outputs":[{"name":"stdout","text":"🏃 View run Feature_Engineering at: https://dagshub.com/gnada22/ml_final_project.mlflow/#/experiments/3/runs/cbeda826e6804f93982bfa6e153b8718\n🧪 View experiment at: https://dagshub.com/gnada22/ml_final_project.mlflow/#/experiments/3\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# print(X_train.columns)\n\nwith mlflow.start_run(run_name=\"Training\"):\n    # grid_search.fit(X_train, y_train)\n    # best_model = grid_search.best_estimator_\n    # preds = best_model.predict(X_val)\n\n    # mlflow.log_params(grid_search.best_params_)\n\n    pipeline.fit(X_train, y_train)\n    preds = pipeline.predict(X_val)\n    best_model = pipeline\n\n    mae = mean_absolute_error(y_val, preds)\n    weights = X_val[\"IsHoliday\"].apply(lambda x: 5 if x else 1)\n    wmae = (weights * np.abs(y_val - preds)).sum() / weights.sum()\n\n    mlflow.log_metric(\"MAE\", mae)\n    mlflow.log_metric(\"WMAE\", wmae)\n\n    model_path = \"model.pkl\"\n    joblib.dump(best_model, model_path)\n    mlflow.log_artifact(model_path)\n    \n    # mlflow.sklearn.log_model(best_model, artifact_path=\"model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-03T17:11:39.574222Z","iopub.execute_input":"2025-07-03T17:11:39.574570Z","iopub.status.idle":"2025-07-03T17:23:09.951355Z","shell.execute_reply.started":"2025-07-03T17:11:39.574548Z","shell.execute_reply":"2025-07-03T17:23:09.949609Z"}},"outputs":[{"name":"stdout","text":"store:  1\nSkipping Store 1, Dept 77 due to insufficient data\nSkipping Store 1, Dept 78 due to insufficient data\nstore:  2\nSkipping Store 2, Dept 39 due to insufficient data\nSkipping Store 2, Dept 77 due to insufficient data\nstore:  3\nSkipping Store 3, Dept 77 due to insufficient data\nSkipping Store 3, Dept 78 due to insufficient data\nstore:  4\nSkipping (4, 47) due to error: LU decomposition error.\nstore:  5\nSkipping Store 5, Dept 51 due to insufficient data\nSkipping Store 5, Dept 77 due to insufficient data\nSkipping Store 5, Dept 78 due to insufficient data\nSkipping Store 5, Dept 80 due to insufficient data\nstore:  6\nSkipping Store 6, Dept 77 due to insufficient data\nstore:  7\nSkipping Store 7, Dept 77 due to insufficient data\nSkipping Store 7, Dept 78 due to insufficient data\nSkipping Store 7, Dept 99 due to insufficient data\nstore:  8\nSkipping Store 8, Dept 51 due to insufficient data\nSkipping Store 8, Dept 77 due to insufficient data\nSkipping Store 8, Dept 96 due to insufficient data\nstore:  9\nSkipping Store 9, Dept 51 due to insufficient data\nSkipping Store 9, Dept 77 due to insufficient data\nSkipping Store 9, Dept 78 due to insufficient data\nSkipping (9, 80) due to error: LU decomposition error.\nSkipping Store 9, Dept 93 due to insufficient data\nstore:  10\nSkipping Store 10, Dept 77 due to insufficient data\nstore:  11\nSkipping Store 11, Dept 48 due to insufficient data\nSkipping Store 11, Dept 50 due to insufficient data\nSkipping Store 11, Dept 77 due to insufficient data\nSkipping Store 11, Dept 78 due to insufficient data\nstore:  12\nSkipping Store 12, Dept 77 due to insufficient data\nstore:  13\nSkipping Store 13, Dept 43 due to insufficient data\nSkipping Store 13, Dept 77 due to insufficient data\nstore:  14\nSkipping Store 14, Dept 43 due to insufficient data\nSkipping Store 14, Dept 77 due to insufficient data\nSkipping Store 14, Dept 78 due to insufficient data\nstore:  15\nSkipping Store 15, Dept 37 due to insufficient data\nSkipping Store 15, Dept 43 due to insufficient data\nSkipping Store 15, Dept 48 due to insufficient data\nSkipping Store 15, Dept 77 due to insufficient data\nSkipping Store 15, Dept 99 due to insufficient data\nstore:  16\nSkipping Store 16, Dept 77 due to insufficient data\nSkipping Store 16, Dept 78 due to insufficient data\nSkipping Store 16, Dept 99 due to insufficient data\nstore:  17\nSkipping Store 17, Dept 47 due to insufficient data\nSkipping Store 17, Dept 77 due to insufficient data\nSkipping Store 17, Dept 78 due to insufficient data\nSkipping Store 17, Dept 99 due to insufficient data\nstore:  18\nSkipping Store 18, Dept 48 due to insufficient data\nSkipping Store 18, Dept 77 due to insufficient data\nSkipping Store 18, Dept 99 due to insufficient data\nstore:  19\nSkipping (19, 48) due to error: LU decomposition error.\nSkipping Store 19, Dept 77 due to insufficient data\nstore:  20\nSkipping Store 20, Dept 78 due to insufficient data\nstore:  21\nSkipping Store 21, Dept 48 due to insufficient data\nSkipping Store 21, Dept 50 due to insufficient data\nSkipping Store 21, Dept 77 due to insufficient data\nSkipping Store 21, Dept 78 due to insufficient data\nSkipping Store 21, Dept 96 due to insufficient data\nSkipping Store 21, Dept 99 due to insufficient data\nstore:  22\nSkipping Store 22, Dept 77 due to insufficient data\nSkipping Store 22, Dept 78 due to insufficient data\nSkipping Store 22, Dept 99 due to insufficient data\nstore:  23\nSkipping Store 23, Dept 77 due to insufficient data\nSkipping Store 23, Dept 99 due to insufficient data\nstore:  24\nSkipping Store 24, Dept 77 due to insufficient data\nstore:  25\nSkipping Store 25, Dept 77 due to insufficient data\nSkipping Store 25, Dept 78 due to insufficient data\nSkipping Store 25, Dept 96 due to insufficient data\nstore:  26\nSkipping Store 26, Dept 77 due to insufficient data\nSkipping Store 26, Dept 78 due to insufficient data\nstore:  27\nSkipping Store 27, Dept 39 due to insufficient data\nstore:  28\nSkipping Store 28, Dept 43 due to insufficient data\nSkipping Store 28, Dept 77 due to insufficient data\nSkipping Store 28, Dept 78 due to insufficient data\nstore:  29\nSkipping Store 29, Dept 43 due to insufficient data\nSkipping Store 29, Dept 45 due to insufficient data\nSkipping (29, 47) due to error: LU decomposition error.\nSkipping Store 29, Dept 48 due to insufficient data\nSkipping Store 29, Dept 77 due to insufficient data\nSkipping Store 29, Dept 78 due to insufficient data\nSkipping Store 29, Dept 96 due to insufficient data\nSkipping Store 29, Dept 99 due to insufficient data\nstore:  30\nSkipping Store 30, Dept 19 due to insufficient data\nSkipping Store 30, Dept 29 due to insufficient data\nSkipping Store 30, Dept 33 due to insufficient data\nstore:  31\nSkipping Store 31, Dept 77 due to insufficient data\nstore:  32\nSkipping Store 32, Dept 77 due to insufficient data\nstore:  33\nSkipping Store 33, Dept 34 due to insufficient data\nSkipping Store 33, Dept 35 due to insufficient data\nSkipping Store 33, Dept 36 due to insufficient data\nSkipping Store 33, Dept 49 due to insufficient data\nSkipping Store 33, Dept 72 due to insufficient data\nSkipping Store 33, Dept 99 due to insufficient data\nstore:  34\nSkipping Store 34, Dept 77 due to insufficient data\nSkipping Store 34, Dept 78 due to insufficient data\nstore:  35\nSkipping Store 35, Dept 45 due to insufficient data\nSkipping Store 35, Dept 77 due to insufficient data\nSkipping Store 35, Dept 78 due to insufficient data\nstore:  36\nSkipping Store 36, Dept 24 due to insufficient data\nSkipping Store 36, Dept 33 due to insufficient data\nSkipping Store 36, Dept 34 due to insufficient data\nSkipping Store 36, Dept 49 due to insufficient data\nSkipping Store 36, Dept 85 due to insufficient data\nSkipping Store 36, Dept 99 due to insufficient data\nstore:  37\nSkipping Store 37, Dept 41 due to insufficient data\nSkipping Store 37, Dept 71 due to insufficient data\nSkipping Store 37, Dept 99 due to insufficient data\nstore:  38\nSkipping (38, 34) due to error: LU decomposition error.\nSkipping Store 38, Dept 35 due to insufficient data\nstore:  39\nSkipping Store 39, Dept 77 due to insufficient data\nSkipping Store 39, Dept 78 due to insufficient data\nstore:  40\nSkipping Store 40, Dept 19 due to insufficient data\nSkipping (40, 47) due to error: LU decomposition error.\nSkipping Store 40, Dept 77 due to insufficient data\nSkipping Store 40, Dept 78 due to insufficient data\nstore:  41\nSkipping Store 41, Dept 37 due to insufficient data\nSkipping Store 41, Dept 77 due to insufficient data\nSkipping Store 41, Dept 78 due to insufficient data\nstore:  42\nSkipping Store 42, Dept 6 due to insufficient data\nSkipping Store 42, Dept 24 due to insufficient data\nSkipping Store 42, Dept 33 due to insufficient data\nSkipping Store 42, Dept 41 due to insufficient data\nSkipping Store 42, Dept 71 due to insufficient data\nstore:  43\nSkipping Store 43, Dept 24 due to insufficient data\nSkipping Store 43, Dept 33 due to insufficient data\nSkipping Store 43, Dept 55 due to insufficient data\nSkipping Store 43, Dept 71 due to insufficient data\nSkipping Store 43, Dept 99 due to insufficient data\nstore:  44\nSkipping Store 44, Dept 34 due to insufficient data\nSkipping (44, 44) due to error: LU decomposition error.\nSkipping (44, 55) due to error: LU decomposition error.\nSkipping Store 44, Dept 71 due to insufficient data\nSkipping Store 44, Dept 99 due to insufficient data\nstore:  45\nSkipping Store 45, Dept 96 due to insufficient data\n🏃 View run Training at: https://dagshub.com/gnada22/ml_final_project.mlflow/#/experiments/3/runs/287cc44618b34b8fb0c36ca949166a21\n🧪 View experiment at: https://dagshub.com/gnada22/ml_final_project.mlflow/#/experiments/3\n","output_type":"stream"}],"execution_count":45}]}