{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3816,"databundleVersionId":32105,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install dagshub mlflow neuralforecast --quiet\n\nimport warnings\nfrom statsmodels.tools.sm_exceptions import ValueWarning\n\nwarnings.filterwarnings(\"ignore\", category=ValueWarning)\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:41:49.997881Z","iopub.execute_input":"2025-07-04T18:41:49.998184Z","iopub.status.idle":"2025-07-04T18:43:18.910526Z","shell.execute_reply.started":"2025-07-04T18:41:49.998160Z","shell.execute_reply":"2025-07-04T18:43:18.909517Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.8/285.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDone!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:43:21.402527Z","iopub.execute_input":"2025-07-04T18:43:21.402881Z","iopub.status.idle":"2025-07-04T18:43:21.413007Z","shell.execute_reply.started":"2025-07-04T18:43:21.402847Z","shell.execute_reply":"2025-07-04T18:43:21.411847Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\n/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import mlflow.sklearn\nfrom datetime import datetime\nimport joblib\nimport dagshub\nimport mlflow\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom tqdm import tqdm\nfrom sklearn.compose import ColumnTransformer\nfrom statsmodels.tsa.arima.model import ARIMA\nimport os\nfrom neuralforecast.models import DLinear\nfrom neuralforecast import NeuralForecast\n\ndagshub.init(repo_owner='gnada22', repo_name='ml_final_project', mlflow=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:43:23.079100Z","iopub.execute_input":"2025-07-04T18:43:23.079433Z","iopub.status.idle":"2025-07-04T18:43:44.977407Z","shell.execute_reply.started":"2025-07-04T18:43:23.079408Z","shell.execute_reply":"2025-07-04T18:43:44.976606Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Initialized MLflow to track repo \u001b[32m\"gnada22/ml_final_project\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"gnada22/ml_final_project\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Repository gnada22/ml_final_project initialized!\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository gnada22/ml_final_project initialized!\n</pre>\n"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# class definitions\n\nclass DateFeatureCreator(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        X[\"week\"] = (\n            X[\"Date\"].dt.to_period(\"W\")\n            .rank(method=\"dense\")\n            .astype(int) - 1\n        )\n        X[\"sin_13\"] = np.sin(2 * np.pi * X[\"week\"] / 13)\n        X[\"cos_13\"] = np.cos(2 * np.pi * X[\"week\"] / 13)\n        X[\"sin_23\"] = np.sin(2 * np.pi * X[\"week\"] / 23)\n        X[\"cos_23\"] = np.cos(2 * np.pi * X[\"week\"] / 23)\n        X = X.drop(columns=[\"Date\"])\n        return X\n\ndate_features = [\"week\", \"sin_13\", \"cos_13\", \"sin_23\", \"cos_23\"]\n\nclass LagFeatureAdder:\n    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n        df = df.sort_values([\"Store\", \"Dept\", \"Date\"])\n        df[\"lag_1\"] = df.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(1)\n        df[\"lag_52\"] = df.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(52)\n        return df\n\nlag_features = [\"lag_1\", \"lag_52\"]\n\nadded_features = date_features + lag_features\n\nclass ColumnTransformerWithNames(ColumnTransformer):\n    def get_feature_names_out(self, input_features=None):\n        return super().get_feature_names_out(input_features)\n\n    def transform(self, X):\n        X_transformed = super().transform(X)\n        # Get feature names for columns\n        cols = self.get_feature_names_out()\n        cols = [c.split(\"__\", 1)[-1] for c in self.get_feature_names_out()]\n        res = pd.DataFrame(X_transformed, columns=cols, index=X.index)\n        # print(\"with name transform - \", type(res))\n        return res\n\n    def fit_transform(self, X, y=None):\n        X_transformed = super().fit_transform(X, y)\n        cols = self.get_feature_names_out()\n        cols = [c.split(\"__\", 1)[-1] for c in self.get_feature_names_out()]\n        res = pd.DataFrame(X_transformed, columns=cols, index=X.index)\n        # print(\"with name fit_transform - \", type(res))\n        return res\n\nclass MultiIndexKeeper(BaseEstimator, TransformerMixin):\n    def __init__(self, index_cols=[\"Date\", \"Store\", \"Dept\"]):\n        self.index_cols = index_cols\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        X.set_index(self.index_cols, drop=False, inplace=True)\n        return X\n\nclass ColumnDropper(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        return X.drop(columns=self.columns, errors=\"ignore\")\n\ndef extract_onehot_value(row, prefix, default=1):\n    for col in row.index:\n        if col.startswith(prefix) and row[col] == 1:\n            return int(col.split(\"_\")[1])\n    return default\n\nclass ResidualRegressor(BaseEstimator, RegressorMixin):\n    def __init__(self, base_model, residual_model):\n        self.base_model = base_model\n        self.residual_model = residual_model\n\n    def fit(self, X, y):\n        self.base_model_ = clone(self.base_model)\n        self.base_model_.fit(X, y)\n        residuals = y - self.base_model_.predict(X)\n        \n        self.residual_model_ = clone(self.residual_model)\n        self.residual_model_.fit(X, residuals)\n        return self\n\n    def extract_onehot_value(self, row, prefix, default=1):\n        for col in row.index:\n            if col.startswith(prefix) and row[col] == 1:\n                return int(col.split(\"_\")[1])\n        return default\n    \n    def predict(self, X):\n        pred_lookup = {}\n        weekly_preds = pd.Series(index=X.index, dtype=float)\n        \n        for week in tqdm(X[\"week\"].sort_values().unique(), desc=\"Recursive prediction\"):\n            day_rows = X[X[\"week\"] == week].copy()\n        \n            for idx, row in day_rows.iterrows():\n                store = extract_onehot_value(row, \"Store_\")\n                dept = extract_onehot_value(row, \"Dept_\")\n        \n                # Get keys for previous lags\n                key_1 = (store, dept, week - 1)\n                key_52 = (store, dept, week - 52)\n        \n                lag_1 = pred_lookup.get(key_1, row[\"lag_1\"])\n                lag_52 = pred_lookup.get(key_52, row[\"lag_52\"])\n        \n                day_rows.at[idx, \"lag_1\"] = lag_1\n                day_rows.at[idx, \"lag_52\"] = lag_52\n        \n            # Predict all rows for this day in one batch\n            y_preds = self.pred_f(day_rows)\n        \n            # Assign predictions back\n            weekly_preds[day_rows.index] = y_preds\n            \n            # Update lookup for future lag access\n            for idx, pred in zip(day_rows.index, y_preds):\n                row = day_rows.loc[idx]\n                \n                store = extract_onehot_value(row, \"Store_\")\n                dept = extract_onehot_value(row, \"Dept_\")\n                \n                key = (store, dept, row[\"week\"])\n                pred_lookup[key] = pred\n    \n        return weekly_preds.fillna(0).to_numpy()\n\n    def pred_f(self, X):\n        return self.base_model_.predict(X) + self.residual_model_.predict(X)\n\nclass ARIMARegressor(BaseEstimator, RegressorMixin):\n    def __init__(self, order=(1, 0, 0), store_level='Store', dept_level='Dept'):\n        self.order = order\n        self.store_level = store_level\n        self.dept_level = dept_level\n\n    def fit(self, X, y):\n        if not isinstance(X.index, pd.MultiIndex):\n            raise ValueError(\"X must have a MultiIndex\")\n\n        self.models_ = {}\n        self.avgs_ = {}\n\n        df = X.copy()\n        df[\"target\"] = y.values\n\n        grouped = df.groupby(level=[self.store_level, self.dept_level])\n\n        for (store, dept), group_df in grouped:\n            if dept == 1:\n                print(\"store: \", store)\n    \n            ts = group_df[\"target\"].copy()\n            exog = group_df.drop(columns=[\"target\"])\n        \n            try:\n                model = ARIMA(endog=ts, order=self.order).fit()\n                self.models_[(store, dept)] = model\n            except Exception as e:\n                # Skip problematic groups\n                print(f\"Skipping ({store}, {dept}) due to error: {e}\")\n                self.skip_(ts, store, dept)\n                continue\n\n        return self\n\n    def skip_(self, ts, store, dept):\n        if ts is None or len(ts) == 0:\n            self.avgs_[(store, dept)] = 0.0\n        else:\n            self.avgs_[(store, dept)] = ts.mean()\n\n    def predict(self, X):\n        if not isinstance(X.index, pd.MultiIndex):\n            raise ValueError(\"X must have a MultiIndex\")\n    \n        preds = pd.Series(index=X.index, dtype=float)\n    \n        # Group X by store-dept pair (based on index levels)\n        grouped = X.groupby(level=[self.store_level, self.dept_level])\n    \n        for (store, dept), group_df in grouped:\n            if dept == 1:\n                print(\"store: \", store)\n            model = self.models_.get((store, dept))\n            if model is None:\n                preds.loc[group_df.index] = self.avgs_.get((store, dept), 0)\n                continue\n\n            # exog = group_df.copy()\n    \n            # Forecast N steps = number of rows in this group\n            forecast = model.forecast(steps=len(group_df))\n            preds.loc[group_df.index] = forecast.to_numpy()\n    \n        return preds.to_numpy()\n\nclass DLinearRegressor(BaseEstimator, RegressorMixin):\n    def __init__(self, input_chunk_length=52, output_chunk_length=39, epochs=10, batch_size=32):\n        self.input_chunk_length = input_chunk_length\n        self.output_chunk_length = output_chunk_length\n        self.epochs = epochs\n        self.batch_size = batch_size\n\n    def fit(self, X, y):\n        df = X.copy()\n        df[\"y\"] = y.values\n\n        if not isinstance(df.index, pd.MultiIndex):\n            raise ValueError(\"X must have a MultiIndex\")\n\n        df = df.reset_index()\n        df.rename(columns={\"Date\": \"ds\"}, inplace=True)\n        df[\"unique_id\"] = df[\"Store\"].astype(str) + \"_\" + df[\"Dept\"].astype(str)\n\n        self.train_df_ = df[[\"unique_id\", \"ds\", \"y\"]].copy()\n\n        # num_samples = len(self.train_df_)\n        # max_steps = int((num_samples / self.batch_size) * self.epochs)\n\n        model = DLinear(\n            input_size=self.input_chunk_length,\n            h=self.output_chunk_length,\n            max_steps=self.epochs * 104,\n            batch_size=self.batch_size,\n            random_seed=42\n        )\n\n        self.nf_ = NeuralForecast(models=[model], freq=\"W-FRI\")\n        self.nf_.fit(df=self.train_df_)\n        return self\n\n    def predict(self, X):\n        df = X.reset_index()\n        df.rename(columns={\"Date\": \"ds\"}, inplace=True)\n        df[\"unique_id\"] = df[\"Store\"].astype(str) + \"_\" + df[\"Dept\"].astype(str)\n\n        forecast_df = self.nf_.predict()\n        forecast_df = forecast_df.rename(columns={\"DLinear\": \"y_hat\"})\n\n        merged = df.merge(forecast_df, on=[\"unique_id\", \"ds\"], how=\"left\")\n        preds = pd.Series(data=merged[\"y_hat\"].fillna(0).values, index=X.index)\n\n        return preds.to_numpy()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:43:47.554665Z","iopub.execute_input":"2025-07-04T18:43:47.554975Z","iopub.status.idle":"2025-07-04T18:43:47.593287Z","shell.execute_reply.started":"2025-07-04T18:43:47.554955Z","shell.execute_reply":"2025-07-04T18:43:47.592374Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"local_path = mlflow.artifacts.download_artifacts(\n    artifact_uri=\"mlflow-artifacts:/e83ef275cec24f3193fe6da7bfedd8b9/d7b6439d6da64738a3b6508250ca6594/artifacts/model.pkl\"\n)\n\nprint(\"Downloaded file size:\", os.path.getsize(local_path) / (1024 ** 2), \"MB\")\n\nmodel = joblib.load(local_path)\n\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:43:52.570282Z","iopub.execute_input":"2025-07-04T18:43:52.570986Z","iopub.status.idle":"2025-07-04T18:43:55.876755Z","shell.execute_reply.started":"2025-07-04T18:43:52.570957Z","shell.execute_reply":"2025-07-04T18:43:55.875590Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c210e966bd2474fa5bb7c613c2b1daf"}},"metadata":{}},{"name":"stdout","text":"Downloaded file size: 16.14536762237549 MB\nDone!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# load and add lag features\n\ntest = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip\", parse_dates=[\"Date\"])\nfeatures = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\", parse_dates=[\"Date\"])\nstores = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\")\n\ndf = test.merge(features, on=[\"Store\", \"Date\", \"IsHoliday\"], how=\"left\")\ndf = df.merge(stores, on=\"Store\", how=\"left\")\n\ndef add_lag_features(df):\n    train = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\", parse_dates=[\"Date\"])\n    train = train[[\"Store\", \"Dept\", \"Date\", \"Weekly_Sales\"]]\n    train = train.sort_values([\"Store\", \"Dept\", \"Date\"])\n    \n    full = pd.concat([train, df], axis=0)\n    full = full.sort_values([\"Store\", \"Dept\", \"Date\"])\n    \n    full[\"lag_1\"] = full.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(1)\n    full[\"lag_52\"] = full.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(52)\n\n    res = full[full[\"Weekly_Sales\"].isna()].copy()\n\n    return res\n\ndf = add_lag_features(df)\n\nX_test = df.drop(columns=[\"Weekly_Sales\"], errors=\"ignore\")\n\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:43:58.904645Z","iopub.execute_input":"2025-07-04T18:43:58.904958Z","iopub.status.idle":"2025-07-04T18:43:59.710572Z","shell.execute_reply.started":"2025-07-04T18:43:58.904934Z","shell.execute_reply":"2025-07-04T18:43:59.709562Z"}},"outputs":[{"name":"stdout","text":"Done!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"preds = model.predict(X_test)\n\nsubmission = pd.DataFrame()\nsubmission[\"Weekly_Sales\"] = preds\nsubmission[\"Id\"] = X_test[\"Store\"].astype(str) + \"_\" + X_test[\"Dept\"].astype(str) + \"_\" + X_test[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n# print(submission)\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"✅ Submission saved as submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-04T18:44:01.961166Z","iopub.execute_input":"2025-07-04T18:44:01.961826Z","iopub.status.idle":"2025-07-04T18:44:20.647543Z","shell.execute_reply.started":"2025-07-04T18:44:01.961796Z","shell.execute_reply":"2025-07-04T18:44:20.646671Z"}},"outputs":[{"name":"stderr","text":"2025-07-04 18:44:04.899042: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751654645.123840      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751654645.191016      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a4deaf27551464c97e7ef9279499dce"}},"metadata":{}},{"name":"stdout","text":"✅ Submission saved as submission.csv\n","output_type":"stream"}],"execution_count":13}]}