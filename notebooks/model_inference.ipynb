{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3816,"databundleVersionId":32105,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install dagshub mlflow --quiet\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T19:55:38.754604Z","iopub.execute_input":"2025-07-01T19:55:38.754832Z","iopub.status.idle":"2025-07-01T19:55:55.894304Z","shell.execute_reply.started":"2025-07-01T19:55:38.754812Z","shell.execute_reply":"2025-07-01T19:55:55.893125Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m733.8/733.8 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.5/95.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nydata-profiling 4.16.1 requires dacite>=1.8, but you have dacite 1.6.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mDone!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-01T19:56:17.243946Z","iopub.execute_input":"2025-07-01T19:56:17.245130Z","iopub.status.idle":"2025-07-01T19:56:17.658000Z","shell.execute_reply.started":"2025-07-01T19:56:17.245082Z","shell.execute_reply":"2025-07-01T19:56:17.657173Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\n/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import mlflow.sklearn\nfrom datetime import datetime\nimport joblib\nimport dagshub\nimport mlflow\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom tqdm import tqdm\n\ndagshub.init(repo_owner='gnada22', repo_name='ml_final_project', mlflow=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T19:56:19.186160Z","iopub.execute_input":"2025-07-01T19:56:19.186790Z","iopub.status.idle":"2025-07-01T19:56:26.496662Z","shell.execute_reply.started":"2025-07-01T19:56:19.186764Z","shell.execute_reply":"2025-07-01T19:56:26.495904Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                                       \u001b[1m❗❗❗ AUTHORIZATION REQUIRED ❗❗❗\u001b[0m                                        \n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">❗❗❗ AUTHORIZATION REQUIRED ❗❗❗</span>                                        \n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n\nOpen the following link in your browser to authorize the client:\nhttps://dagshub.com/login/oauth/authorize?state=18c09f31-155f-4f12-9b6b-2db54f8a5f76&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=f3c0c7ff394546cdc3e8a74d0f3f14609e40b575d0e2e89e2bd7830d28fce37e\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Accessing as gnada22\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as gnada22\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Initialized MLflow to track repo \u001b[32m\"gnada22/ml_final_project\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"gnada22/ml_final_project\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Repository gnada22/ml_final_project initialized!\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository gnada22/ml_final_project initialized!\n</pre>\n"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"class DateFeatureCreator(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        X[\"weekofyear\"] = X[\"Date\"].dt.isocalendar().week.astype(int)\n        X[\"sin_13\"] = np.sin(2 * np.pi * X[\"weekofyear\"] / 13)\n        X[\"cos_13\"] = np.cos(2 * np.pi * X[\"weekofyear\"] / 13)\n        X[\"sin_23\"] = np.sin(2 * np.pi * X[\"weekofyear\"] / 23)\n        X[\"cos_23\"] = np.cos(2 * np.pi * X[\"weekofyear\"] / 23)\n        X = X.drop(columns=[\"Date\"])\n        return X\n\nlocal_path = mlflow.artifacts.download_artifacts(\n    artifact_uri=\"mlflow-artifacts:/bdaee488c065473c96d054e5ae43539a/abb40eeefeb54f7a87763486cb01f016/artifacts/model.pkl\"\n)\n\nmodel = joblib.load(local_path)\n\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T19:56:29.292547Z","iopub.execute_input":"2025-07-01T19:56:29.292887Z","iopub.status.idle":"2025-07-01T19:56:30.318429Z","shell.execute_reply.started":"2025-07-01T19:56:29.292841Z","shell.execute_reply":"2025-07-01T19:56:30.317533Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75771cf8a72e4c9a860743d962b659d3"}},"metadata":{}},{"name":"stdout","text":"Done!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# load and add lag features\n\ntest = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip\", parse_dates=[\"Date\"])\nfeatures = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\", parse_dates=[\"Date\"])\nstores = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\")\n\ndf = test.merge(features, on=[\"Store\", \"Date\", \"IsHoliday\"], how=\"left\")\ndf = df.merge(stores, on=\"Store\", how=\"left\")\n\ndef add_lag_features(df):\n    train = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\", parse_dates=[\"Date\"])\n    train = train[[\"Store\", \"Dept\", \"Date\", \"Weekly_Sales\"]]\n    train = train.sort_values([\"Store\", \"Dept\", \"Date\"])\n    \n    full = pd.concat([train, df], axis=0)\n    full = full.sort_values([\"Store\", \"Dept\", \"Date\"])\n    \n    full[\"lag_1\"] = full.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(1)\n    full[\"lag_52\"] = full.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(52)\n\n    res = full[full[\"Weekly_Sales\"].isna()].copy() # just test rows\n    # res.dropna(subset=[\"lag_1\", \"lag_52\"], inplace=True)  # drop rows with missing lags\n\n    return res\n\ndf = add_lag_features(df)\n\nX_test = df.drop(columns=[\"Weekly_Sales\"], errors=\"ignore\")\n\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T19:56:45.054093Z","iopub.execute_input":"2025-07-01T19:56:45.054508Z","iopub.status.idle":"2025-07-01T19:56:45.794512Z","shell.execute_reply.started":"2025-07-01T19:56:45.054478Z","shell.execute_reply":"2025-07-01T19:56:45.793567Z"}},"outputs":[{"name":"stdout","text":"Done!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"X_test = X_test.sort_values([\"Date\", \"Store\", \"Dept\"]).reset_index(drop=True)\nX_test[\"Weekly_Sales_Predicted\"] = np.nan\n\n# Create a lookup table for easier lag value access\npred_lookup = {}\n\n# Iterate by day\nfor date in tqdm(X_test[\"Date\"].sort_values().unique(), desc=\"Recursive prediction\"):\n    day_rows = X_test[X_test[\"Date\"] == date].copy()\n\n    for idx, row in day_rows.iterrows():\n        store = row[\"Store\"]\n        dept = row[\"Dept\"]\n\n        # Get keys for previous lags\n        key_1 = (store, dept, date - pd.Timedelta(weeks=1))\n        key_52 = (store, dept, date - pd.Timedelta(weeks=52))\n\n        lag_1 = pred_lookup.get(key_1, row[\"lag_1\"])\n        lag_52 = pred_lookup.get(key_52, row[\"lag_52\"])\n\n        # print(store, dept, date, lag_1)\n\n        day_rows.at[idx, \"lag_1\"] = lag_1\n        day_rows.at[idx, \"lag_52\"] = lag_52\n\n    # Predict all rows for this day in one batch\n    y_preds = model.predict(day_rows)\n\n    # Assign predictions back\n    X_test.loc[day_rows.index, \"Weekly_Sales_Predicted\"] = y_preds\n    # print(X_test.loc[day_rows.index, [\"Weekly_Sales_Predicted\", \"Date\", \"Store\", \"Dept\"]])\n    # print(y_preds)\n\n    # Update lookup for future lag access\n    for idx, pred in zip(day_rows.index, y_preds):\n        row = day_rows.loc[idx]\n        key = (row[\"Store\"], row[\"Dept\"], row[\"Date\"])\n        pred_lookup[key] = pred\n\n# X_test = X_test.sort_values([\"Store\", \"Dept\", \"Date\"])\n# print(X_test[[\"Store\", \"Dept\", \"Date\", \"Weekly_Sales_Predicted\"]])\n\nsubmission[\"Weekly_Sales\"] = X_test[\"Weekly_Sales_Predicted\"]\nsubmission[\"Id\"] = X_test[\"Store\"].astype(str) + \"_\" + X_test[\"Dept\"].astype(str) + \"_\" + X_test[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n# print(submission)\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"✅ Submission saved as submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T20:37:31.562733Z","iopub.execute_input":"2025-07-01T20:37:31.563451Z","iopub.status.idle":"2025-07-01T20:37:59.092906Z","shell.execute_reply.started":"2025-07-01T20:37:31.563425Z","shell.execute_reply":"2025-07-01T20:37:59.091714Z"}},"outputs":[{"name":"stderr","text":"Recursive prediction: 100%|██████████| 39/39 [00:27<00:00,  1.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"✅ Submission saved as submission.csv\n","output_type":"stream"}],"execution_count":28}]}