{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3816,"databundleVersionId":32105,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install dagshub mlflow neuralforecast --quiet\n\nimport warnings\nfrom statsmodels.tools.sm_exceptions import ValueWarning\n\nwarnings.filterwarnings(\"ignore\", category=ValueWarning)\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T10:43:38.093334Z","iopub.execute_input":"2025-07-05T10:43:38.093976Z","iopub.status.idle":"2025-07-05T10:45:04.196005Z","shell.execute_reply.started":"2025-07-05T10:43:38.093949Z","shell.execute_reply":"2025-07-05T10:45:04.195358Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-05T10:45:09.006769Z","iopub.execute_input":"2025-07-05T10:45:09.007274Z","iopub.status.idle":"2025-07-05T10:45:09.014081Z","shell.execute_reply.started":"2025-07-05T10:45:09.007242Z","shell.execute_reply":"2025-07-05T10:45:09.013373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import mlflow.sklearn\nfrom datetime import datetime\nimport joblib\nimport dagshub\nimport mlflow\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom tqdm import tqdm\nfrom sklearn.compose import ColumnTransformer\nfrom statsmodels.tsa.arima.model import ARIMA\nimport os\nfrom neuralforecast.models import DLinear, PatchTST\nfrom neuralforecast import NeuralForecast\n\ndagshub.init(repo_owner='gnada22', repo_name='ml_final_project', mlflow=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T10:45:10.612554Z","iopub.execute_input":"2025-07-05T10:45:10.613097Z","iopub.status.idle":"2025-07-05T10:45:36.680769Z","shell.execute_reply.started":"2025-07-05T10:45:10.613073Z","shell.execute_reply":"2025-07-05T10:45:36.680165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# class definitions\n\nclass DateFeatureCreator(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        X[\"week\"] = (\n            X[\"Date\"].dt.to_period(\"W\")\n            .rank(method=\"dense\")\n            .astype(int) - 1\n        )\n        X[\"sin_13\"] = np.sin(2 * np.pi * X[\"week\"] / 13)\n        X[\"cos_13\"] = np.cos(2 * np.pi * X[\"week\"] / 13)\n        X[\"sin_23\"] = np.sin(2 * np.pi * X[\"week\"] / 23)\n        X[\"cos_23\"] = np.cos(2 * np.pi * X[\"week\"] / 23)\n        X = X.drop(columns=[\"Date\"])\n        return X\n\ndate_features = [\"week\", \"sin_13\", \"cos_13\", \"sin_23\", \"cos_23\"]\n\nclass LagFeatureAdder:\n    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n        df = df.sort_values([\"Store\", \"Dept\", \"Date\"])\n        df[\"lag_1\"] = df.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(1)\n        df[\"lag_52\"] = df.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(52)\n        return df\n\nlag_features = [\"lag_1\", \"lag_52\"]\n\nadded_features = date_features + lag_features\n\nclass ColumnTransformerWithNames(ColumnTransformer):\n    def get_feature_names_out(self, input_features=None):\n        return super().get_feature_names_out(input_features)\n\n    def transform(self, X):\n        X_transformed = super().transform(X)\n        # Get feature names for columns\n        cols = self.get_feature_names_out()\n        cols = [c.split(\"__\", 1)[-1] for c in self.get_feature_names_out()]\n        res = pd.DataFrame(X_transformed, columns=cols, index=X.index)\n        # print(\"with name transform - \", type(res))\n        return res\n\n    def fit_transform(self, X, y=None):\n        X_transformed = super().fit_transform(X, y)\n        cols = self.get_feature_names_out()\n        cols = [c.split(\"__\", 1)[-1] for c in self.get_feature_names_out()]\n        res = pd.DataFrame(X_transformed, columns=cols, index=X.index)\n        # print(\"with name fit_transform - \", type(res))\n        return res\n\nclass MultiIndexKeeper(BaseEstimator, TransformerMixin):\n    def __init__(self, index_cols=[\"Date\", \"Store\", \"Dept\"]):\n        self.index_cols = index_cols\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        X.set_index(self.index_cols, drop=False, inplace=True)\n        return X\n\nclass ColumnDropper(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        return X.drop(columns=self.columns, errors=\"ignore\")\n\ndef extract_onehot_value(row, prefix, default=1):\n    for col in row.index:\n        if col.startswith(prefix) and row[col] == 1:\n            return int(col.split(\"_\")[1])\n    return default\n\nclass ResidualRegressor(BaseEstimator, RegressorMixin):\n    def extract_onehot_value(self, row, prefix, default=1):\n        for col in row.index:\n            if col.startswith(prefix) and row[col] == 1:\n                return int(col.split(\"_\")[1])\n        return default\n    \n    def predict(self, X):\n        pred_lookup = {}\n        weekly_preds = pd.Series(index=X.index, dtype=float)\n        \n        for week in tqdm(X[\"week\"].sort_values().unique(), desc=\"Recursive prediction\"):\n            day_rows = X[X[\"week\"] == week].copy()\n        \n            for idx, row in day_rows.iterrows():\n                store = extract_onehot_value(row, \"Store_\")\n                dept = extract_onehot_value(row, \"Dept_\")\n        \n                # Get keys for previous lags\n                key_1 = (store, dept, week - 1)\n                key_52 = (store, dept, week - 52)\n        \n                lag_1 = pred_lookup.get(key_1, row[\"lag_1\"])\n                lag_52 = pred_lookup.get(key_52, row[\"lag_52\"])\n        \n                day_rows.at[idx, \"lag_1\"] = lag_1\n                day_rows.at[idx, \"lag_52\"] = lag_52\n        \n            # Predict all rows for this day in one batch\n            y_preds = self.pred_f(day_rows)\n        \n            # Assign predictions back\n            weekly_preds[day_rows.index] = y_preds\n            \n            # Update lookup for future lag access\n            for idx, pred in zip(day_rows.index, y_preds):\n                row = day_rows.loc[idx]\n                \n                store = extract_onehot_value(row, \"Store_\")\n                dept = extract_onehot_value(row, \"Dept_\")\n                \n                key = (store, dept, row[\"week\"])\n                pred_lookup[key] = pred\n    \n        return weekly_preds.fillna(0).to_numpy()\n\n    def pred_f(self, X):\n        return self.base_model_.predict(X) + self.residual_model_.predict(X)\n\nclass ARIMARegressor(BaseEstimator, RegressorMixin):\n    def predict(self, X):\n        if not isinstance(X.index, pd.MultiIndex):\n            raise ValueError(\"X must have a MultiIndex\")\n    \n        preds = pd.Series(index=X.index, dtype=float)\n    \n        # Group X by store-dept pair (based on index levels)\n        grouped = X.groupby(level=[self.store_level, self.dept_level])\n    \n        for (store, dept), group_df in grouped:\n            if dept == 1:\n                print(\"store: \", store)\n            model = self.models_.get((store, dept))\n            if model is None:\n                preds.loc[group_df.index] = self.avgs_.get((store, dept), 0)\n                continue\n\n            # exog = group_df.copy()\n    \n            # Forecast N steps = number of rows in this group\n            forecast = model.forecast(steps=len(group_df))\n            preds.loc[group_df.index] = forecast.to_numpy()\n    \n        return preds.to_numpy()\n\nclass DLinearRegressor(BaseEstimator, RegressorMixin):\n    def predict(self, X):\n        df = X.reset_index()\n        df.rename(columns={\"Date\": \"ds\"}, inplace=True)\n        df[\"unique_id\"] = df[\"Store\"].astype(str) + \"_\" + df[\"Dept\"].astype(str)\n\n        forecast_df = self.nf_.predict()\n        forecast_df = forecast_df.rename(columns={\"DLinear\": \"y_hat\"})\n\n        merged = df.merge(forecast_df, on=[\"unique_id\", \"ds\"], how=\"left\")\n        preds = pd.Series(data=merged[\"y_hat\"].fillna(0).values, index=X.index)\n\n        return preds.to_numpy()\n\nclass PatchTSTRegressor(BaseEstimator, RegressorMixin):\n    def predict(self, X):\n        df = X.reset_index()\n        df.rename(columns={\"Date\": \"ds\"}, inplace=True)\n        df[\"unique_id\"] = df[\"Store\"].astype(str) + \"_\" + df[\"Dept\"].astype(str)\n\n        forecast_df = self.nf_.predict()\n        forecast_df = forecast_df.rename(columns={\"PatchTST\": \"y_hat\"})\n\n        merged = df.merge(forecast_df, on=[\"unique_id\", \"ds\"], how=\"left\")\n        preds = pd.Series(data=merged[\"y_hat\"].fillna(0).values, index=X.index)\n\n        return preds.to_numpy()\n\nclass NBEATSRegressor(BaseEstimator, RegressorMixin):\n    def predict(self, X):\n        df = X.reset_index()\n        df.rename(columns={\"Date\": \"ds\"}, inplace=True)\n        df[\"unique_id\"] = df[\"Store\"].astype(str) + \"_\" + df[\"Dept\"].astype(str)\n\n        forecast_df = self.nf_.predict()\n        forecast_df = forecast_df.rename(columns={\"NBEATS\": \"y_hat\"})\n\n        merged = df.merge(forecast_df, on=[\"unique_id\", \"ds\"], how=\"left\")\n        preds = pd.Series(data=merged[\"y_hat\"].fillna(0).values, index=X.index)\n\n        return preds.to_numpy()\n\nclass TFTRegressor(BaseEstimator, RegressorMixin):\n    def predict(self, X):\n        df = X.reset_index()\n        df.rename(columns={\"Date\": \"ds\"}, inplace=True)\n        df[\"unique_id\"] = df[\"Store\"].astype(str) + \"_\" + df[\"Dept\"].astype(str)\n\n        forecast_df = self.nf_.predict()\n        forecast_df = forecast_df.rename(columns={\"TFT\": \"y_hat\"})\n\n        merged = df.merge(forecast_df, on=[\"unique_id\", \"ds\"], how=\"left\")\n        preds = pd.Series(data=merged[\"y_hat\"].fillna(0).values, index=X.index)\n\n        return preds.to_numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T10:45:39.374011Z","iopub.execute_input":"2025-07-05T10:45:39.374387Z","iopub.status.idle":"2025-07-05T10:45:39.406924Z","shell.execute_reply.started":"2025-07-05T10:45:39.374360Z","shell.execute_reply":"2025-07-05T10:45:39.406241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"local_path = mlflow.artifacts.download_artifacts(\n    artifact_uri=\"mlflow-artifacts:/4e741da8978446259b80b5b149310c3c/ad8aa507a9334013997c9f09376cfa21/artifacts/model.pkl\"\n)\n\nprint(\"Downloaded file size:\", os.path.getsize(local_path) / (1024 ** 2), \"MB\")\n\nmodel = joblib.load(local_path)\n\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T10:45:43.934749Z","iopub.execute_input":"2025-07-05T10:45:43.935034Z","iopub.status.idle":"2025-07-05T10:45:48.075424Z","shell.execute_reply.started":"2025-07-05T10:45:43.935009Z","shell.execute_reply":"2025-07-05T10:45:48.074632Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load and add lag features\n\ntest = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip\", parse_dates=[\"Date\"])\nfeatures = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\", parse_dates=[\"Date\"])\nstores = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\")\n\ndf = test.merge(features, on=[\"Store\", \"Date\", \"IsHoliday\"], how=\"left\")\ndf = df.merge(stores, on=\"Store\", how=\"left\")\n\ndef add_lag_features(df):\n    train = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\", parse_dates=[\"Date\"])\n    train = train[[\"Store\", \"Dept\", \"Date\", \"Weekly_Sales\"]]\n    train = train.sort_values([\"Store\", \"Dept\", \"Date\"])\n    \n    full = pd.concat([train, df], axis=0)\n    full = full.sort_values([\"Store\", \"Dept\", \"Date\"])\n    \n    full[\"lag_1\"] = full.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(1)\n    full[\"lag_52\"] = full.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(52)\n\n    res = full[full[\"Weekly_Sales\"].isna()].copy()\n\n    return res\n\ndf = add_lag_features(df)\n\nX_test = df.drop(columns=[\"Weekly_Sales\"], errors=\"ignore\")\n\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T10:45:49.922961Z","iopub.execute_input":"2025-07-05T10:45:49.923546Z","iopub.status.idle":"2025-07-05T10:45:51.029621Z","shell.execute_reply.started":"2025-07-05T10:45:49.923522Z","shell.execute_reply":"2025-07-05T10:45:51.028934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"preds = model.predict(X_test)\n\nsubmission = pd.DataFrame()\nsubmission[\"Weekly_Sales\"] = preds\nsubmission[\"Id\"] = X_test[\"Store\"].astype(str) + \"_\" + X_test[\"Dept\"].astype(str) + \"_\" + X_test[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n# print(submission)\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"✅ Submission saved as submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T10:45:52.800820Z","iopub.execute_input":"2025-07-05T10:45:52.801362Z","iopub.status.idle":"2025-07-05T10:46:08.439559Z","shell.execute_reply.started":"2025-07-05T10:45:52.801337Z","shell.execute_reply":"2025-07-05T10:46:08.438917Z"}},"outputs":[],"execution_count":null}]}
