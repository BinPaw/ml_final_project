{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3816,"databundleVersionId":32105,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install dagshub mlflow --quiet\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T15:00:16.450591Z","iopub.execute_input":"2025-07-02T15:00:16.450823Z","iopub.status.idle":"2025-07-02T15:00:31.053077Z","shell.execute_reply.started":"2025-07-02T15:00:16.450794Z","shell.execute_reply":"2025-07-02T15:00:31.052027Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m733.8/733.8 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.5/95.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nydata-profiling 4.16.1 requires dacite>=1.8, but you have dacite 1.6.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mDone!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-02T15:00:33.779553Z","iopub.execute_input":"2025-07-02T15:00:33.780060Z","iopub.status.idle":"2025-07-02T15:00:34.093115Z","shell.execute_reply.started":"2025-07-02T15:00:33.780028Z","shell.execute_reply":"2025-07-02T15:00:34.092290Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\n/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, FunctionTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nimport mlflow\nimport mlflow.sklearn\nimport dagshub\nimport joblib\nimport os\nfrom sklearn.model_selection import GridSearchCV\nfrom lightgbm import LGBMRegressor\n\ndagshub.init(repo_owner='gnada22', repo_name='ml_final_project', mlflow=True)\n\nmlflow.set_experiment(\"LightGBM_Training\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T15:08:50.235304Z","iopub.execute_input":"2025-07-02T15:08:50.235611Z","iopub.status.idle":"2025-07-02T15:08:50.579388Z","shell.execute_reply.started":"2025-07-02T15:08:50.235588Z","shell.execute_reply":"2025-07-02T15:08:50.578714Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Initialized MLflow to track repo \u001b[32m\"gnada22/ml_final_project\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"gnada22/ml_final_project\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Repository gnada22/ml_final_project initialized!\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository gnada22/ml_final_project initialized!\n</pre>\n"},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<Experiment: artifact_location='mlflow-artifacts:/0a121ae8035d4c4f8e403176c374cd78', creation_time=1751468776707, experiment_id='2', last_update_time=1751468776707, lifecycle_stage='active', name='LightGBM_Training', tags={}>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"class DateFeatureCreator(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        X[\"weekofyear\"] = X[\"Date\"].dt.isocalendar().week.astype(int)\n        X[\"sin_13\"] = np.sin(2 * np.pi * X[\"weekofyear\"] / 13)\n        X[\"cos_13\"] = np.cos(2 * np.pi * X[\"weekofyear\"] / 13)\n        X[\"sin_23\"] = np.sin(2 * np.pi * X[\"weekofyear\"] / 23)\n        X[\"cos_23\"] = np.cos(2 * np.pi * X[\"weekofyear\"] / 23)\n        X = X.drop(columns=[\"Date\"])\n        return X\n\ndate_features = [\"weekofyear\", \"sin_13\", \"cos_13\", \"sin_23\", \"cos_23\"]\n\nclass LagFeatureAdder:\n    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n        df = df.sort_values([\"Store\", \"Dept\", \"Date\"])\n        df[\"lag_1\"] = df.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(1)\n        df[\"lag_52\"] = df.groupby([\"Store\", \"Dept\"])[\"Weekly_Sales\"].shift(52)\n        return df\n\nlag_features = [\"lag_1\", \"lag_52\"]\n\nadded_features = date_features + lag_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T15:08:51.842777Z","iopub.execute_input":"2025-07-02T15:08:51.843064Z","iopub.status.idle":"2025-07-02T15:08:51.853866Z","shell.execute_reply.started":"2025-07-02T15:08:51.843041Z","shell.execute_reply":"2025-07-02T15:08:51.853040Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class ResidualRegressor(BaseEstimator, RegressorMixin):\n    def __init__(self, base_model, residual_model):\n        self.base_model = base_model\n        self.residual_model = residual_model\n\n    def fit(self, X, y):\n        self.base_model_ = clone(self.base_model)\n        self.base_model_.fit(X, y)\n        residuals = y - self.base_model_.predict(X)\n        \n        self.residual_model_ = clone(self.residual_model)\n        self.residual_model_.fit(X, residuals)\n        return self\n\n    def predict(self, X):\n        return self.base_model_.predict(X) + self.residual_model_.predict(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T15:08:53.124158Z","iopub.execute_input":"2025-07-02T15:08:53.124443Z","iopub.status.idle":"2025-07-02T15:08:53.129794Z","shell.execute_reply.started":"2025-07-02T15:08:53.124422Z","shell.execute_reply":"2025-07-02T15:08:53.128990Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Load and merge data\ndf_train = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\", parse_dates=[\"Date\"])\ndf_features = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\", parse_dates=[\"Date\"])\ndf_stores = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\")\n\ndf = df_train.merge(df_features, on=[\"Store\", \"Date\", \"IsHoliday\"], how=\"left\")\ndf = df.merge(df_stores, on=\"Store\", how=\"left\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T15:08:54.434679Z","iopub.execute_input":"2025-07-02T15:08:54.435182Z","iopub.status.idle":"2025-07-02T15:08:54.842470Z","shell.execute_reply.started":"2025-07-02T15:08:54.435159Z","shell.execute_reply":"2025-07-02T15:08:54.841778Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"categorical = [\"Store\", \"Dept\", \"Type\", \"IsHoliday\"]\nnumerical = [\"Temperature\", \"Fuel_Price\", \"CPI\", \"Unemployment\",\n             \"MarkDown1\", \"MarkDown2\", \"MarkDown3\", \"MarkDown4\", \"MarkDown5\"]\nengineered = added_features\n\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\nnumerical_transformer = SimpleImputer(strategy=\"mean\")\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"num\", numerical_transformer, numerical + engineered),\n        (\"cat\", categorical_transformer, categorical),\n    ]\n)\n\nresidual_model = ResidualRegressor(\n    base_model=LinearRegression(),\n    residual_model=LGBMRegressor(n_estimators=100, max_depth=3)\n)\n\npipeline = Pipeline([\n    (\"date_features\", DateFeatureCreator()),\n    (\"preprocessor\", preprocessor),\n    (\"regressor\", residual_model)\n])\n\nparam_grid = {\n    \"regressor__residual_model__n_estimators\": [50, 100, 200],\n    \"regressor__residual_model__max_depth\": [3, 5, 7],\n    \"regressor__residual_model__learning_rate\": [0.01, 0.1, 0.2]\n}\n\ngrid_search = GridSearchCV(\n    estimator=pipeline,\n    param_grid=param_grid,\n    scoring=\"neg_mean_absolute_error\",\n    cv=3,\n    n_jobs=-1,\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T15:08:56.394142Z","iopub.execute_input":"2025-07-02T15:08:56.394398Z","iopub.status.idle":"2025-07-02T15:08:56.401419Z","shell.execute_reply.started":"2025-07-02T15:08:56.394380Z","shell.execute_reply":"2025-07-02T15:08:56.400710Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"print(df.columns)\n\nwith mlflow.start_run(run_name=\"Feature_Engineering\"):\n    lag_adder = LagFeatureAdder()\n    df = lag_adder.transform(df)\n    original_rows = len(df)\n    df = df.dropna(subset=[\"lag_1\", \"lag_52\"])\n    remaining_rows = len(df)\n    mlflow.log_param(\"rows_before_dropna\", original_rows)\n    mlflow.log_param(\"rows_after_dropna\", remaining_rows)\n    mlflow.log_param(\"features_added\", added_features)\n\ny = df[\"Weekly_Sales\"]\nX = df.drop(columns=[\"Weekly_Sales\"])\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T15:08:59.939597Z","iopub.execute_input":"2025-07-02T15:08:59.940319Z","iopub.status.idle":"2025-07-02T15:09:01.030460Z","shell.execute_reply.started":"2025-07-02T15:08:59.940292Z","shell.execute_reply":"2025-07-02T15:09:01.029574Z"}},"outputs":[{"name":"stdout","text":"Index(['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Temperature',\n       'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4',\n       'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size'],\n      dtype='object')\n🏃 View run Feature_Engineering at: https://dagshub.com/gnada22/ml_final_project.mlflow/#/experiments/2/runs/52e7b9cf86794b32842f25053108f66e\n🧪 View experiment at: https://dagshub.com/gnada22/ml_final_project.mlflow/#/experiments/2\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# print(X_train.columns)\n\nwith mlflow.start_run(run_name=\"Training\"):\n    grid_search.fit(X_train, y_train)\n    best_model = grid_search.best_estimator_\n    preds = best_model.predict(X_val)\n\n    mlflow.log_params(grid_search.best_params_)\n\n    mae = mean_absolute_error(y_val, preds)\n    weights = X_val[\"IsHoliday\"].apply(lambda x: 5 if x else 1)\n    wmae = (weights * np.abs(y_val - preds)).sum() / weights.sum()\n\n    mlflow.log_metric(\"MAE\", mae)\n    mlflow.log_metric(\"WMAE\", wmae)\n    # mlflow.sklearn.log_model(best_model, artifact_path=\"model\")\n\n    model_path = \"model.pkl\"\n    joblib.dump(best_model, model_path)\n    mlflow.log_artifact(model_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}